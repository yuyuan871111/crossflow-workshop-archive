{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossflow.tasks import SubprocessTask\n",
    "from crossflow.clients import Client\n",
    "from dask_jobqueue import SLURMCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/e280/e280-workshop/yuyang/myenv/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 38457 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cluster = SLURMCluster(cores=1, \n",
    "                       job_cpu=1,\n",
    "                       processes=1,\n",
    "                       memory='256GB',\n",
    "                       queue='standard',\n",
    "                       job_directives_skip=['--mem', '-n '],\n",
    "                       interface='hsn0',\n",
    "                       job_extra_directives=['--nodes=1',\n",
    "                           '--reservation=e280-workshop_1018917',\n",
    "                           '--qos=reservation', \n",
    "                           '--tasks-per-node=128'],\n",
    "                       python='python',\n",
    "                       account='e280-workshop',\n",
    "                       walltime=\"06:00:00\",\n",
    "                       shebang=\"#!/bin/bash --login\",\n",
    "                       local_directory='$PWD',\n",
    "                       job_script_prologue=['module load gromacs',\n",
    "                                  'export OMP_NUM_THREADS=1',\n",
    "                                  'source /work/e280/e280-workshop/yuyang/myenv/bin/activate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "cluster.scale(1) # start a single worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURMCluster(1605d29c, 'tcp://10.253.46.22:42921', workers=1, threads=1, memory=222.00 GiB)\n"
     ]
    }
   ],
   "source": [
    "print(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the 1ake endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grompp = SubprocessTask('gmx grompp -f x.mdp -c x.gro -p x.top -o system.tpr -maxwarn 1')\n",
    "grompp.set_inputs(['x.mdp', 'x.gro', 'x.top'])\n",
    "grompp.set_outputs(['system.tpr'])\n",
    "\n",
    "mdrun = SubprocessTask('srun --distribution=block:block --hint=nomultithread gmx_mpi mdrun  -deffnm system')\n",
    "mdrun.set_inputs(['system.tpr'])\n",
    "mdrun.set_outputs(['system.log', 'system.trr'])\n",
    "\n",
    "makewhole = SubprocessTask('echo 0 | gmx trjconv -f broken.trr -s system.tpr -o whole.trr -pbc whole')\n",
    "makewhole.set_inputs(['broken.trr', 'system.tpr'])\n",
    "makewhole.set_outputs(['whole.trr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Future: pending, key: lambda-37a2bc2894235edeca226540c7ed24fc>\n",
      "<Future: pending, key: lambda-e7e1cf50eedac23424d2583dd636d547>\n",
      "<Future: pending, key: run-684c0391-e829-4b33-b3e8-bd167abe25e0>\n"
     ]
    }
   ],
   "source": [
    "tprA = client.submit(grompp, 'nvt.mdp', '1ake_em.gro', '1ake.top')\n",
    "logA, trajA = client.submit(mdrun, tprA)\n",
    "wholeA = client.submit(makewhole, trajA, tprA)\n",
    "print(logA)\n",
    "print(trajA)\n",
    "print(wholeA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1ake_test.trr'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logA.result().save('1ake_test.log')\n",
    "wholeA.result().save('1ake_test.trr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossflow.filehandling import FileHandler\n",
    "fh = FileHandler()\n",
    "startcrdsA = fh.load('1ake_em.gro')\n",
    "topA = fh.load('1ake.top')\n",
    "mdp = fh.load('nvt.mdp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grompp job 2 completed\n",
      "grompp job 8 completed\n",
      "grompp job 1 completed\n",
      "grompp job 0 completed\n",
      "grompp job 9 completed\n",
      "grompp job 3 completed\n",
      "mdrun job 2 completed\n",
      "make_whole job 2 completed\n",
      "grompp job 5 completed\n",
      "mdrun job 1 completed\n",
      "mdrun job 9 completed\n",
      "make_whole job 1 completed\n",
      "make_whole job 9 completed\n",
      "mdrun job 8 completed\n",
      "make_whole job 8 completed\n",
      "grompp job 6 completed\n",
      "grompp job 7 completed\n",
      "grompp job 4 completed\n",
      "mdrun job 6 completed\n",
      "mdrun job 7 completed\n",
      "make_whole job 6 completed\n",
      "mdrun job 0 completed\n",
      "mdrun job 3 completed\n",
      "make_whole job 7 completed\n",
      "mdrun job 4 completed\n",
      "make_whole job 4 completed\n",
      "mdrun job 5 completed\n",
      "make_whole job 0 completed\n",
      "make_whole job 3 completed\n",
      "make_whole job 5 completed\n"
     ]
    }
   ],
   "source": [
    "from distributed import wait, as_completed\n",
    "n_reps = 10\n",
    "startcrdsAs = [startcrdsA] * n_reps # replicate the starting structure\n",
    "tprAs = client.map(grompp, mdp, startcrdsAs, topA)\n",
    "logAs, trajAs = client.map(mdrun, tprAs)\n",
    "wholeAs = client.map(makewhole, trajAs)\n",
    "for future in as_completed(tprAs + trajAs + wholeAs):\n",
    "    if future in tprAs:\n",
    "        print(f'grompp job {tprAs.index(future)} completed')\n",
    "    elif future in trajAs:\n",
    "        print(f'mdrun job {trajAs.index(future)} completed')\n",
    "    else:\n",
    "        print(f'make_whole job {wholeAs.index(future)} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the 4ake endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "startcrdsB = fh.load('4ake_em.gro')\n",
    "topB = fh.load('4ake.top')\n",
    "\n",
    "cluster.scale(4) # Reset the size of the cluster to a moderate value\n",
    "tprB = client.submit(grompp, mdp, startcrdsB, topB)\n",
    "logB, trajB = client.submit(mdrun, tprB)\n",
    "wholeB = client.submit(makewhole, trajB, tprB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as mdt\n",
    "\n",
    "ensembleA = mdt.load(wholeA.result(), top=startcrdsA)\n",
    "ensembleB = mdt.load(wholeB.result(), top=startcrdsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mdplus.pca import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def rmsd2(ensembleA, ensembleB, sel):\n",
    "    \"\"\"\n",
    "    Calculate approximate 2D RMSD matrix via PCA\n",
    "\n",
    "    Args:\n",
    "\n",
    "        ensembleA (mdtraj trajectory): ensemble A of nA frames\n",
    "        ensembleB (mdtraj trajectory): ensemble B of nB frames\n",
    "        sel (string): mdtraj selection specifier for RMSD calculation\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        rmsd2d (numpy array[nA, nB]): RMSD matrix\n",
    "    \"\"\"\n",
    "    idxA = ensembleA.topology.select(sel)\n",
    "    idxB = ensembleB.topology.select(sel)\n",
    "    x = np.concatenate([ensembleA.xyz[:,idxA], ensembleB.xyz[:, idxB]])\n",
    "    p = PCA()\n",
    "    scores = p.fit_transform(x)\n",
    "    d = cdist(scores[:len(ensembleA)], scores[len(ensembleA):])\n",
    "    return d / np.sqrt(len(idxA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_distances(ensembleA, ensembleB, selection='name CA', max_pairs=10):\n",
    "    '''\n",
    "    Calculate the RMSD distances between all snapshots in each of two ensembles and return the closest\n",
    "\n",
    "    Args:\n",
    "       ensembleA (mdtraj trajectory): first ensemble\n",
    "       ensembleB (mdtraj trajectory): second ensemble\n",
    "       selection (string): mdtraj selection for RMSD calculation\n",
    "       max_pairs (int): number of closest pairs to return\n",
    "\n",
    "    Returns:\n",
    "       pairlist: sorted dictionary with tuple of snaphot indices as keys, RMSDs as values\n",
    "    '''\n",
    "    pairdist = {}\n",
    "    d = rmsd2(ensembleA, ensembleB, selection)\n",
    "    for i in range(ensembleA.n_frames):\n",
    "        for j in range(ensembleB.n_frames):\n",
    "            key = (i, j)\n",
    "            pairdist[key] = d[i, j]\n",
    "        \n",
    "    # sort by increasing RMSD:\n",
    "    pairdist = {k:v for k, v in sorted(pairdist.items(), key=lambda i: i[1])[:max_pairs]}\n",
    "    return pairdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 8): 0.7100329136073691, (0, 9): 0.7110713569172359, (0, 7): 0.7130101411327233, (0, 0): 0.7151266022313763, (0, 6): 0.716788622193025}\n"
     ]
    }
   ],
   "source": [
    "# Now run it:\n",
    "closest_pairs = get_pair_distances(ensembleA, ensembleB, max_pairs=5)\n",
    "print(closest_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cycle 0...\n",
      "MD runs finished, finding closest pairs...\n",
      "Cycle 0: closest pairs = {(45, 35): 0.6757456151493262, (30, 35): 0.6782170197500583, (32, 35): 0.6784142521007794, (31, 35): 0.6807478663070266, (26, 35): 0.6812686199036484}\n",
      "Starting cycle 1...\n",
      "MD runs finished, finding closest pairs...\n",
      "Cycle 1: closest pairs = {(105, 80): 0.6455366545591223, (106, 80): 0.6482641749946749, (105, 116): 0.6499597005582308, (104, 80): 0.6501932100793748, (103, 80): 0.6504947367456139}\n",
      "Starting cycle 2...\n",
      "MD runs finished, finding closest pairs...\n",
      "Cycle 2: closest pairs = {(105, 148): 0.6375325320816776, (121, 148): 0.6375493175191181, (143, 148): 0.637549317546703, (105, 131): 0.6386808669186296, (143, 131): 0.6386977400689988}\n",
      "Starting cycle 3...\n",
      "MD runs finished, finding closest pairs...\n",
      "Cycle 3: closest pairs = {(218, 219): 0.56417450617941, (219, 219): 0.566036488928158, (215, 219): 0.5665269341855169, (214, 219): 0.5706958725524629, (105, 219): 0.5716329409663421}\n",
      "Starting cycle 4...\n",
      "MD runs finished, finding closest pairs...\n",
      "Cycle 4: closest pairs = {(265, 268): 0.5573253621144217, (231, 268): 0.5573463476756969, (218, 268): 0.5573802495212402, (266, 268): 0.5581041926350021, (243, 268): 0.5589199490348107}\n",
      "Starting cycle 5...\n",
      "MD runs finished, finding closest pairs...\n",
      "Cycle 5: closest pairs = {(314, 295): 0.5139768273942629, (314, 294): 0.5150927361252329, (314, 293): 0.5189636577150019, (314, 296): 0.5197375799963679, (313, 295): 0.5199683385221936}\n",
      "Starting cycle 6...\n",
      "MD runs finished, finding closest pairs...\n",
      "Cycle 6: closest pairs = {(354, 366): 0.501487314293109, (354, 354): 0.5035351928156147, (352, 366): 0.5041414773003814, (363, 366): 0.5041414773003814, (374, 366): 0.504141477302939}\n",
      "Starting cycle 7...\n",
      "MD runs finished, finding closest pairs...\n",
      "Cycle 7: closest pairs = {(410, 450): 0.4777923845638435, (409, 450): 0.48148087822326463, (410, 448): 0.48235450945392366, (407, 450): 0.4836815396874778, (396, 450): 0.4836815396979323}\n",
      "Starting cycle 8...\n",
      "MD runs finished, finding closest pairs...\n",
      "Cycle 8: closest pairs = {(410, 452): 0.46268404046979655, (451, 452): 0.46269270093922293, (473, 452): 0.4626927009448039, (474, 452): 0.4635250509766507, (462, 452): 0.4667940432657468}\n",
      "Starting cycle 9...\n",
      "MD runs finished, finding closest pairs...\n",
      "Cycle 9: closest pairs = {(410, 521): 0.45955203243254283, (517, 521): 0.4595582274925723, (528, 521): 0.4595582274933374, (451, 521): 0.4595590187220189, (506, 521): 0.4595590187232997}\n",
      "Path search completed\n"
     ]
    }
   ],
   "source": [
    "max_cycles = 10 # Maximum number of workflow iterations\n",
    "min_rmsd = 0.2 # Target minimum RMSD between structures from each ensemble\n",
    "max_pairs = 5 # Number of shortest inter-ensemble pairs to take forward to next iteration\n",
    "cluster.scale(max_pairs) # Scale the SLURMCluster up to max_pairs workers\n",
    "\n",
    "for icycle in range(max_cycles):\n",
    "    print(f'Starting cycle {icycle}...')\n",
    "    shortestA = [k[0] for k in list(closest_pairs.keys())] # indices of chosen structures from A\n",
    "    shortestB = [k[1] for k in list(closest_pairs.keys())] # ditto for B\n",
    "    tprAs = client.map(grompp, mdp, [ensembleA[i] for i in shortestA], topA) # run grompp jobs in parallel\n",
    "    tprBs = client.map(grompp, mdp, [ensembleB[i] for i in shortestB], topB) # ditto\n",
    "    logAs, trajAs = client.map(mdrun, tprAs) # run MD jobs in parallel\n",
    "    logBs, trajBs = client.map(mdrun, tprBs) # ditto\n",
    "    wholeAs = client.map(makewhole, trajAs, tprAs) # remove PBC artifacts\n",
    "    wholeBs = client.map(makewhole, trajBs, tprBs)\n",
    "    wait(wholeBs) # Block here until all jobs are done\n",
    "    print('MD runs finished, finding closest pairs...')\n",
    "    ensembleA += mdt.load([t.result() for t in wholeAs], top='1ake_em.gro') # Add to ensembles\n",
    "    ensembleB += mdt.load([t.result() for t in wholeBs], top='4ake_em.gro')\n",
    "    closest_pairs = get_pair_distances(ensembleA, ensembleB, max_pairs=max_pairs)\n",
    "    print(f'Cycle {icycle}: closest pairs = {closest_pairs}')\n",
    "    if list(closest_pairs.values())[0] < min_rmsd: # The two ends of the path have \"met\" - stop.\n",
    "        break\n",
    "        \n",
    "print('Path search completed')\n",
    "cluster.scale(0) # Scale the cluster back to having no workers at all.\n",
    "ensembleA.save('ensembleA.xtc') # Save the ensembles in Gromacs xtc format for later analysis.\n",
    "ensembleB.save('ensembleB.xtc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
